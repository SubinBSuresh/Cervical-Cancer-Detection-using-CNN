# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UgR5KXAM7uplhmO2eQUsI_OEsC7QaxM7
"""

# Import Libraries

import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from IPython.display import Image
from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization, ZeroPadding2D, Convolution2D, MaxPool2D
from keras.applications.vgg16 import VGG16
from keras import models
import tensorflow.keras.backend as K 
from tensorflow.keras.models import Model
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pathlib

from tensorflow.keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D, Input, Lambda,LeakyReLU, MaxPooling2D, Multiply, Permute, Reshape, UpSampling2D

# Preparing the data

path = "/home/subin/PROJECT/DATASETS/Sipakmed/TRAIN"
data_dir = pathlib.Path(path)


# Determining the size of the data or dataset

batch_size = 4
img_height = 224 
img_width = 224 

# checking the total number of images in the file 

image_count = len(list(data_dir.glob('*/*.jpg')))
print("Total Images:",image_count)

# Determining the size of the data or dataset

batch_size = 4 #3
img_height = 224 #180
img_width = 224 #180

# Training Dataset

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.5,
subset="training",
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

#Validation Dataset

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.5,
subset="validation",
seed=100,
image_size=(img_height, img_width),
batch_size=batch_size)

# Showing our classes

class_names = train_ds.class_names
print(class_names)

# Show 9 images of predictive data.

plt.figure(figsize=(7, 7))
for images, labels in train_ds.take(1):
 for i in range(9):
   ax = plt.subplot(3, 3, i + 1)
   plt.imshow(images[i].numpy().astype("uint8"))
   plt.title(class_names[labels[i]])
   plt.axis("off")

# Normalize to convert the color values according to CNN.

normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)
normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
print(np.min(first_image), np.max(first_image))

# Define Model

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',
input_shape=(128,128, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(512, (3, 3), activation='relu'))
model.add(layers.Conv2D(512, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))

# Compile Model

model.compile(optimizer='Adagrad',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
model.summary()

# Add Model Checkpoint Callback

filepath="/home/subin/PROJECT/Checkpoints/Custom/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5"
model_checkpoint_callback = ModelCheckpoint(filepath,monitor='val_accuracy',verbose=1, save_best_only=True, mode='max')

# Add Model EarlyStopping Callback

model_earlystopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor="val_accuracy",
    min_delta=0,
    patience=7,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=False,
)

#Training the model

epochs=30
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,  
  callbacks = [model_checkpoint_callback, model_earlystopping_callback]
)



# Saving the model

model=save_model('/home/subin/PROJECT/Saved_Models/custom.h5')


# Visualizing Training Results

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(epochs)
plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
